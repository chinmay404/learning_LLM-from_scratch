{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmay404/learning_LLM-from_scratch/blob/main/Tokeniser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YrPNeQ_e4lPJ"
      },
      "outputs": [],
      "source": [
        "corp =  \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "In 2025, AI-driven systems will reshape how we work and live.\n",
        "“To be, or not to be”: that is the question.\n",
        "Our new product—EdgeXplore™—launches next quarter!\n",
        "I can’t believe it’s already May; time flies.\n",
        "Découvrez la beauté de la langue française.\n",
        "今日はいい天気ですね。  # Japanese: “It’s nice weather today.”\n",
        "Data-driven insights: 42% of businesses report higher ROI.\n",
        "User123: “How do I reset my password?” asked via support ticket.\n",
        "Fintech, biotech, and cleantech startups are booming.\n",
        "Once upon a midnight dreary, while I pondered, weak and weary…\n",
        "SQL query example: SELECT * FROM users WHERE active = TRUE;\n",
        "¡Hola! ¿Cómo estás? — Spanish greeting exchange.\n",
        "Café-au-lait, crème brûlée, façade, naïve: French loanwords in English.\n",
        "Blockchain-based voting systems promise enhanced security.\n",
        "“Why so serious?” he asked, flicking his Joker card.\n",
        "Machine-learning models achieve state-of-the-art accuracy.\n",
        "RegEx patterns like `\\w+@\\w+\\.\\w+` match basic email addresses.\n",
        "The mitochondrion is the powerhouse of the cell.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'In', '2025', ',', 'AI-driven', 'systems', 'will', 'reshape', 'how', 'we', 'work', 'and', 'live', '.', '“', 'To', 'be', ',', 'or', 'not', 'to', 'be', '”', ':', 'that', 'is', 'the', 'question', '.', 'Our', 'new', 'product—EdgeXplore™—launches', 'next', 'quarter', '!', 'I', 'can', '’', 't', 'believe', 'it', '’', 's', 'already', 'May', ';', 'time', 'flies', '.', 'Découvrez', 'la', 'beauté', 'de', 'la', 'langue', 'française', '.', '今日はいい天気ですね。', '#', 'Japanese', ':', '“', 'It', '’', 's', 'nice', 'weather', 'today.', '”', 'Data-driven', 'insights', ':', '42', '%', 'of', 'businesses', 'report', 'higher', 'ROI', '.', 'User123', ':', '“', 'How', 'do', 'I', 'reset', 'my', 'password', '?', '”', 'asked', 'via', 'support', 'ticket', '.', 'Fintech', ',', 'biotech', ',', 'and', 'cleantech', 'startups', 'are', 'booming', '.', 'Once', 'upon', 'a', 'midnight', 'dreary', ',', 'while', 'I', 'pondered', ',', 'weak', 'and', 'weary…', 'SQL', 'query', 'example', ':', 'SELECT', '*', 'FROM', 'users', 'WHERE', 'active', '=', 'TRUE', ';', '¡Hola', '!', '¿Cómo', 'estás', '?', '—', 'Spanish', 'greeting', 'exchange', '.', 'Café-au-lait', ',', 'crème', 'brûlée', ',', 'façade', ',', 'naïve', ':', 'French', 'loanwords', 'in', 'English', '.', 'Blockchain-based', 'voting', 'systems', 'promise', 'enhanced', 'security', '.', '“', 'Why', 'so', 'serious', '?', '”', 'he', 'asked', ',', 'flicking', 'his', 'Joker', 'card', '.', 'Machine-learning', 'models', 'achieve', 'state-of-the-art', 'accuracy', '.', 'RegEx', 'patterns', 'like', '`', '\\\\w+', '@', '\\\\w+\\\\.\\\\w+', '`', 'match', 'basic', 'email', 'addresses', '.', 'The', 'mitochondrion', 'is', 'the', 'powerhouse', 'of', 'the', 'cell', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(corp)\n",
        "print(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
            "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from transformers) (2.2.3)\n",
            "Requirement already satisfied: packaging>=20.0 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from transformers) (25.0)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
            "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: colorama in s:\\miniconda\\envs\\ml\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
            "  Using cached charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
            "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Using cached charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
            "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Installing collected packages: urllib3, safetensors, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 idna-3.10 pyyaml-6.0.2 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3 urllib3-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "s:\\miniconda\\envs\\ML\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'in', '202', '##5', ',', 'ai', '-', 'driven', 'systems', 'will', 'res', '##ha', '##pe', 'how', 'we', 'work', 'and', 'live', '.', '“', 'to', 'be', ',', 'or', 'not', 'to', 'be', '”', ':', 'that', 'is', 'the', 'question', '.', 'our', 'new', 'product', '—', 'edge', '##x', '##pl', '##ore', '##™', '—', 'launches', 'next', 'quarter', '!', 'i', 'can', '’', 't', 'believe', 'it', '’', 's', 'already', 'may', ';', 'time', 'flies', '.', 'deco', '##u', '##vre', '##z', 'la', 'beau', '##te', 'de', 'la', 'lang', '##ue', 'francaise', '.', '[UNK]', '日', 'は', '##い', '##い', '天', '[UNK]', 'て', '##す', '##ね', '。', '#', 'japanese', ':', '“', 'it', '’', 's', 'nice', 'weather', 'today', '.', '”', 'data', '-', 'driven', 'insights', ':', '42', '%', 'of', 'businesses', 'report', 'higher', 'roi', '.', 'user', '##12', '##3', ':', '“', 'how', 'do', 'i', 'reset', 'my', 'password', '?', '”', 'asked', 'via', 'support', 'ticket', '.', 'fin', '##tech', ',', 'bio', '##tech', ',', 'and', 'clean', '##tech', 'startup', '##s', 'are', 'booming', '.', 'once', 'upon', 'a', 'midnight', 'dr', '##ear', '##y', ',', 'while', 'i', 'pondered', ',', 'weak', 'and', 'weary', '…', 'sql', 'query', 'example', ':', 'select', '*', 'from', 'users', 'where', 'active', '=', 'true', ';', '¡', 'ho', '##la', '!', '¿', 'como', 'est', '##as', '?', '—', 'spanish', 'greeting', 'exchange', '.', 'cafe', '-', 'au', '-', 'lai', '##t', ',', 'cr', '##eme', 'br', '##ule', '##e', ',', 'facade', ',', 'naive', ':', 'french', 'loan', '##words', 'in', 'english', '.', 'block', '##chai', '##n', '-', 'based', 'voting', 'systems', 'promise', 'enhanced', 'security', '.', '“', 'why', 'so', 'serious', '?', '”', 'he', 'asked', ',', 'flicking', 'his', 'joker', 'card', '.', 'machine', '-', 'learning', 'models', 'achieve', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', '.', 'reg', '##ex', 'patterns', 'like', '`', '\\\\', 'w', '+', '@', '\\\\', 'w', '+', '\\\\', '.', '\\\\', 'w', '+', '`', 'match', 'basic', 'email', 'addresses', '.', 'the', 'mit', '##och', '##ond', '##rion', 'is', 'the', 'powerhouse', 'of', 'the', 'cell', '.']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.tokenize(corp)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['running', 'study', 'better']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'studies', 'better']\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Byte-Pair Encoding\n",
        "We'll follow the actual algorithm used in early NLP models (like GPT-2). This includes:\n",
        "\n",
        "Counting symbol pairs\n",
        "\n",
        "Merging the most frequent pair\n",
        "\n",
        "Creating a vocabulary of subwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QUICK VISUAL EXAMPLE:\n",
        "Let’s say your dataset has these words:\n",
        "\n",
        "low\n",
        "lower\n",
        "lowest\n",
        "Initial:\n",
        "\n",
        "l o w\n",
        "l o w e r\n",
        "l o w e s t\n",
        "Now count pairs:\n",
        "\n",
        "“l o” = 2\n",
        "\n",
        "“o w” = 3 ← most common\n",
        "\n",
        "Merge “o w” → “ow”\n",
        "\n",
        "Now you have:\n",
        "\n",
        "l ow\n",
        "l ow e r\n",
        "l ow e s t\n",
        "Keep doing this until you get chunks like:\n",
        "\n",
        "“low”\n",
        "\n",
        "“er”\n",
        "\n",
        "“est”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finds all adjacent pairs of symbols in a word. bcs BPE merges the most frequent pairs, so we must first find all pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question:\n",
        "Why 2 Not 3 or More : \n",
        " ans by GPT : Because Byte Pair Encoding (BPE) is based on the idea of merging two symbols at a time, not more.\n",
        "\n",
        "    🔧 The Original Idea\n",
        "    BPE comes from an old data compression trick (from the 1990s) where:\n",
        "\n",
        "    You count the most common pair of bytes\n",
        "\n",
        "    Merge them into a new byte\n",
        "\n",
        "    Repeat\n",
        "\n",
        "    This same idea was brought into NLP to build subword units.\n",
        "\n",
        "    So at every step:\n",
        "\n",
        "    You only merge 2 things (a pair)\n",
        "\n",
        "    Over time, those 2-letter merges can become longer:\n",
        "\n",
        "    'l' + 'o' = 'lo'\n",
        "\n",
        "    'lo' + 'w' = 'low'\n",
        "\n",
        "    'low' + 'e' = 'lowe'\n",
        "\n",
        "    etc.\n",
        "\n",
        "    We never look for triplets or longer patterns directly, because:\n",
        "\n",
        "    It's too slow (millions of combinations!)\n",
        "\n",
        "    Merging pairs is enough to build up long words gradually\n",
        "\n",
        "    📏 Why not use length-3 or 4?\n",
        "    Because BPE builds up longer sequences step-by-step.\n",
        "    We start with characters, merge common pairs → now some tokens are big chunks → we keep merging those.\n",
        "\n",
        "    This gives us:\n",
        "\n",
        "    Full words (like \"low\")\n",
        "\n",
        "    Parts of words (like \"ing\", \"tion\", \"pre\")\n",
        "\n",
        "    💡 And it works really well without needing to jump to 3+ length merges.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('H', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_pairs(word):\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "get_pairs('Hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_bpe(corpus, num_merges):\n",
        "    # corpus = {\"word\": frequency}\n",
        "    vocab = {tuple(word) + ('</w>',): freq for word, freq in corpus.items()}  # This function learns which symbol pairs to merge, based on how often they appear\n",
        "    merges = []\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        # Count all symbol pairs\n",
        "        pairs = defaultdict(int)\n",
        "        for word, freq in vocab.items():\n",
        "            for pair in get_pairs(word):\n",
        "                pairs[pair] += freq\n",
        "\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Most frequent pair\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        merges.append(best_pair)\n",
        "\n",
        "        # Merge best pair\n",
        "        new_vocab = {}\n",
        "        for word, freq in vocab.items():\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and (word[i], word[i + 1]) == best_pair:\n",
        "                    new_word.append(word[i] + word[i + 1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_vocab[tuple(new_word)] = freq\n",
        "        vocab = new_vocab\n",
        "\n",
        "    return merges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'corp' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sample corpus (word: frequency)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      7\u001b[0m }\n\u001b[1;32m----> 9\u001b[0m merges \u001b[38;5;241m=\u001b[39m train_bpe(build_corpus(\u001b[43mcorp\u001b[49m), num_merges\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearned merges:\u001b[39m\u001b[38;5;124m\"\u001b[39m, merges)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'corp' is not defined"
          ]
        }
      ],
      "source": [
        "# Sample corpus (word: frequency)\n",
        "corpus = {\n",
        "    \"low\": 5,\n",
        "    \"lower\": 2,\n",
        "    \"newest\": 6,\n",
        "    \"widest\": 3\n",
        "}\n",
        "\n",
        "merges = train_bpe(build_corpus(corp), num_merges=10)\n",
        "print(\"Learned merges:\", merges)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_bpe(word, merges):\n",
        "    word = list(word) + ['</w>']\n",
        "    for merge in merges:\n",
        "        i = 0\n",
        "        while i < len(word) - 1:\n",
        "            if (word[i], word[i + 1]) == merge:\n",
        "                word[i:i + 2] = [word[i] + word[i + 1]]\n",
        "            else:\n",
        "                i += 1\n",
        "    return word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized: ['newest</w>']\n"
          ]
        }
      ],
      "source": [
        "new_word = \"newest\"\n",
        "tokens = apply_bpe(new_word, merges)\n",
        "print(\"Tokenized:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Can i Build My Own Corpus and Hows its Built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word counts: Counter({'low': 3, 'lowest': 2, 'newest': 2, 'lower': 1, 'widest': 1, 'new': 1})\n",
            "Counter({'low': 3, 'lowest': 2, 'newest': 2, 'lower': 1, 'widest': 1, 'new': 1})\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def build_corpus(text):\n",
        "    # Step 1: Preprocess the text (lowercase and split into words)\n",
        "    text = text.lower()  # Converts everything to lowercase\n",
        "    words = re.findall(r'\\w+', text)  # Extract words using regex (ignores punctuation)\n",
        "\n",
        "    # Step 2: Count the frequency of each word\n",
        "    word_counts = Counter(words)\n",
        "    print(\"Word counts:\", word_counts)\n",
        "    return word_counts\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"low low lower lowest newest widest lowest new low newest\"\"\"\n",
        "\n",
        "# Build corpus\n",
        "corpus = build_corpus(text)\n",
        "\n",
        "# Convert Counter to the format you want (word: frequency)\n",
        "print(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word counts: Counter({'the': 7, 'and': 3, 'i': 3, 'of': 3, 'w': 3, 'in': 2, 'driven': 2, 'systems': 2, 'how': 2, 'to': 2, 'be': 2, 'is': 2, 'it': 2, 's': 2, 'la': 2, 'asked': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'lazy': 1, 'dog': 1, '2025': 1, 'ai': 1, 'will': 1, 'reshape': 1, 'we': 1, 'work': 1, 'live': 1, 'or': 1, 'not': 1, 'that': 1, 'question': 1, 'our': 1, 'new': 1, 'product': 1, 'edgexplore': 1, 'launches': 1, 'next': 1, 'quarter': 1, 'can': 1, 't': 1, 'believe': 1, 'already': 1, 'may': 1, 'time': 1, 'flies': 1, 'découvrez': 1, 'beauté': 1, 'de': 1, 'langue': 1, 'française': 1, '今日はいい天気ですね': 1, 'japanese': 1, 'nice': 1, 'weather': 1, 'today': 1, 'data': 1, 'insights': 1, '42': 1, 'businesses': 1, 'report': 1, 'higher': 1, 'roi': 1, 'user123': 1, 'do': 1, 'reset': 1, 'my': 1, 'password': 1, 'via': 1, 'support': 1, 'ticket': 1, 'fintech': 1, 'biotech': 1, 'cleantech': 1, 'startups': 1, 'are': 1, 'booming': 1, 'once': 1, 'upon': 1, 'a': 1, 'midnight': 1, 'dreary': 1, 'while': 1, 'pondered': 1, 'weak': 1, 'weary': 1, 'sql': 1, 'query': 1, 'example': 1, 'select': 1, 'from': 1, 'users': 1, 'where': 1, 'active': 1, 'true': 1, 'hola': 1, 'cómo': 1, 'estás': 1, 'spanish': 1, 'greeting': 1, 'exchange': 1, 'café': 1, 'au': 1, 'lait': 1, 'crème': 1, 'brûlée': 1, 'façade': 1, 'naïve': 1, 'french': 1, 'loanwords': 1, 'english': 1, 'blockchain': 1, 'based': 1, 'voting': 1, 'promise': 1, 'enhanced': 1, 'security': 1, 'why': 1, 'so': 1, 'serious': 1, 'he': 1, 'flicking': 1, 'his': 1, 'joker': 1, 'card': 1, 'machine': 1, 'learning': 1, 'models': 1, 'achieve': 1, 'state': 1, 'art': 1, 'accuracy': 1, 'regex': 1, 'patterns': 1, 'like': 1, 'match': 1, 'basic': 1, 'email': 1, 'addresses': 1, 'mitochondrion': 1, 'powerhouse': 1, 'cell': 1})\n",
            "Learned merges: [('e', '</w>'), ('s', '</w>'), ('t', '</w>'), ('e', 'r'), ('i', 'n'), ('a', 'n'), ('y', '</w>'), ('c', 'h'), ('r', 'e'), ('d', '</w>')]\n"
          ]
        }
      ],
      "source": [
        "merges = train_bpe(build_corpus(corp), num_merges=10)\n",
        "print(\"Learned merges:\", merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized: ['n', 'e', 'w', 'e', 's', 't</w>']\n"
          ]
        }
      ],
      "source": [
        "new_word = \"newest\"\n",
        "tokens = apply_bpe(new_word, merges)\n",
        "print(\"Tokenized:\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why its Diffrent Than Previous One :\n",
        "ans By GPT : \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lets Use Hugging Face BPE Tokeniser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in s:\\miniconda\\envs\\ml\\lib\\site-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from tokenizers) (0.30.2)\n",
            "Requirement already satisfied: filelock in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
            "Requirement already satisfied: colorama in s:\\miniconda\\envs\\ml\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in s:\\miniconda\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://huggingface.co/docs/tokenizers/en/api/pre-tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import BPEDecoder\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.decoder = BPEDecoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Pre-tokenization\n",
        "Pre-tokenization is the initial step where the input text is split into smaller units, typically words or subwords, before applying the main tokenization model. This step is crucial for defining how the tokenizer perceives boundaries between tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = ByteLevel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "trainer = BpeTrainer(\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.train(files=[\"alice_in_wonderland.txt\"], trainer=trainer)\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Hello, how are you?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ĠH', 'ell', 'o', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n"
          ]
        }
      ],
      "source": [
        "output = tokenizer.encode(text)\n",
        "print(output.tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NOw use a Pre-trained BPE Tokenizer\n",
        "Hugging Face provides a variety of pre-trained tokenizers that use BPE or similar algorithms like WordPiece. You can load one of these models using the AutoTokenizer class, which automatically selects the appropriate tokenizer based on the pre-trained model you specify.\n",
        "\n",
        "Here’s how you can load a pre-trained tokenizer (such as from the GPT-2 or BERT models, which use BPE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer (for example, GPT-2 uses BPE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize and get the subword tokens\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why does Ġ appear?\n",
        "In the Byte-Level BPE tokenizer, the Ġ character is used to denote that the token (e.g., how, are, you) is not starting a new sentence or word without a preceding space. So, if you see Ġhow, it means that how was preceded by a space in the original text.\n",
        "\n",
        "For example:\n",
        "\n",
        "Hello, → No leading space before the token Hello.\n",
        "\n",
        "Ġhow → A space precedes the token how in the original sentence.\n",
        "\n",
        "In summary:\n",
        "Ġ represents a leading space or whitespace at the beginning of the token.\n",
        "\n",
        "It helps to indicate that the token is not isolated but is part of the ongoing sentence with a space before it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.']\n"
          ]
        }
      ],
      "source": [
        "# Sample sentence to tokenize\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## WordPiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Does WordPiece Work?\n",
        "WordPiece is quite similar to BPE but with some key differences. Let's break it down into simple steps:\n",
        "\n",
        "Preprocessing the Corpus:\n",
        "\n",
        "Tokenize the text into characters.\n",
        "\n",
        "Start with a character-level vocabulary (each character is a token).\n",
        "\n",
        "Count Frequency of Subword Pairs:\n",
        "\n",
        "Like BPE, you count the frequency of pairs of characters (or subwords) and merge the most frequent pairs.\n",
        "\n",
        "Merging Most Frequent Pairs:\n",
        "\n",
        "Merge the most frequent pairs into a new subword token.\n",
        "\n",
        "Repeat this process until you've reached a predefined vocabulary size or some other stopping condition.\n",
        "\n",
        "Vocabulary:\n",
        "\n",
        "After training, the vocabulary is made up of subwords (e.g., \"un\", \"happiness\", \"##ing\", etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: {('low',): 3, ('lower',): 1, ('lowest',): 2, ('newest',): 2, ('widest',): 1, ('new',): 1}\n",
            "Merges: [('l', 'o'), ('lo', 'w'), ('e', 's'), ('es', 't'), ('n', 'e'), ('ne', 'w'), ('low', 'est'), ('new', 'est'), ('low', 'e'), ('lowe', 'r'), ('w', 'i'), ('wi', 'd'), ('wid', 'est')]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def build_wordpiece_vocab(text, vocab_size):\n",
        "    # Step 1: Tokenize the text into words\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\w+', text)\n",
        "    \n",
        "    # Step 2: Split each word into characters\n",
        "    vocab = {}\n",
        "    for word in words:\n",
        "        word_chars = list(word)\n",
        "        if tuple(word_chars) in vocab:\n",
        "            vocab[tuple(word_chars)] += 1\n",
        "        else:\n",
        "            vocab[tuple(word_chars)] = 1\n",
        "    \n",
        "    # Step 3: Create WordPiece merges\n",
        "    merges = []\n",
        "    while len(vocab) < vocab_size:\n",
        "        # Find the most frequent pair of characters\n",
        "        pairs = Counter()\n",
        "        for word in vocab:\n",
        "            for i in range(len(word) - 1):\n",
        "                pair = (word[i], word[i + 1])\n",
        "                pairs[pair] += vocab[word]\n",
        "        \n",
        "        if not pairs:  # If no pairs found, break out of the loop\n",
        "            break\n",
        "        \n",
        "        # Get the most frequent pair and merge it\n",
        "        most_common_pair = pairs.most_common(1)[0][0]\n",
        "        merges.append(most_common_pair)\n",
        "        \n",
        "        # Update vocab: merge the pair in all words\n",
        "        new_vocab = {}\n",
        "        for word in vocab:\n",
        "            new_word = []\n",
        "            skip = False\n",
        "            for i in range(len(word) - 1):\n",
        "                if skip:\n",
        "                    skip = False\n",
        "                    continue\n",
        "                if (word[i], word[i + 1]) == most_common_pair:\n",
        "                    new_word.append(word[i] + word[i + 1])\n",
        "                    skip = True\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "            if not skip:\n",
        "                new_word.append(word[-1])\n",
        "            new_vocab[tuple(new_word)] = vocab[word]\n",
        "        \n",
        "        vocab = new_vocab\n",
        "    \n",
        "    return vocab, merges\n",
        "\n",
        "# Example text\n",
        "text = \"low low lower lowest newest widest lowest new low newest\"\n",
        "vocab_size = 10\n",
        "\n",
        "# Build WordPiece vocabulary\n",
        "vocab, merges = build_wordpiece_vocab(text, vocab_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Merges:\", merges)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEoIFnbnjf+rTF4z16hYR7",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
